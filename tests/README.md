# Lucky Kangaroo - Tests

Ce rÃ©pertoire contient tous les tests de l'application Lucky Kangaroo.

## ğŸ—ï¸ Structure des tests

```
tests/
â”œâ”€â”€ __init__.py              # Package de tests
â”œâ”€â”€ conftest.py              # Configuration pytest et fixtures
â”œâ”€â”€ test_schemas.py          # Tests des schÃ©mas de validation
â”œâ”€â”€ test_api.py              # Tests d'intÃ©gration API
â”œâ”€â”€ test_models.py           # Tests des modÃ¨les de donnÃ©es
â”œâ”€â”€ test_services.py         # Tests des services mÃ©tier
â”œâ”€â”€ test_utils.py            # Tests des utilitaires
â””â”€â”€ README.md                # Cette documentation
```

## ğŸš€ ExÃ©cution des tests

### PrÃ©requis

1. **Python 3.8+** installÃ©
2. **pytest** installÃ© : `pip install -r requirements-test.txt`
3. **Environnement virtuel** activÃ© (recommandÃ©)

### Commandes de base

```bash
# Tous les tests
pytest

# Tests avec couverture
pytest --cov=backend --cov-report=html

# Tests spÃ©cifiques
pytest tests/test_api.py
pytest tests/test_schemas.py

# Tests par marqueurs
pytest -m unit          # Tests unitaires
pytest -m integration   # Tests d'intÃ©gration
pytest -m api           # Tests d'API
pytest -m database      # Tests de base de donnÃ©es
pytest -m slow          # Tests lents (optionnel)
pytest -m external      # Tests de services externes (optionnel)

# Tests en parallÃ¨le
pytest -n auto

# Tests avec rapport HTML
pytest --html=reports/report.html
```

### Scripts PowerShell

```powershell
# Tous les tests
.\scripts\run-tests.ps1

# Tests unitaires avec couverture
.\scripts\run-tests.ps1 -TestType "unit" -Coverage "true"

# Tests d'API en mode verbeux
.\scripts\run-tests.ps1 -TestType "api" -Verbose "true"

# Tests en parallÃ¨le
.\scripts\run-tests.ps1 -Parallel "true"
```

## ğŸ§ª Types de tests

### Tests unitaires (`@pytest.mark.unit`)
- **Objectif** : Tester une fonction ou classe isolÃ©ment
- **PortÃ©e** : Code mÃ©tier, utilitaires, schÃ©mas
- **Exemples** : Validation de donnÃ©es, calculs, transformations

### Tests d'intÃ©gration (`@pytest.mark.integration`)
- **Objectif** : Tester l'interaction entre composants
- **PortÃ©e** : Services, modÃ¨les, base de donnÃ©es
- **Exemples** : CrÃ©ation d'utilisateur, gestion des annonces

### Tests d'API (`@pytest.mark.api`)
- **Objectif** : Tester les endpoints HTTP
- **PortÃ©e** : Routes, contrÃ´leurs, rÃ©ponses
- **Exemples** : Authentification, CRUD des ressources

### Tests de base de donnÃ©es (`@pytest.mark.database`)
- **Objectif** : Tester les opÃ©rations de persistance
- **PortÃ©e** : ModÃ¨les, migrations, requÃªtes
- **Exemples** : Relations, contraintes, performances

### Tests lents (`@pytest.mark.slow`)
- **Objectif** : Tests nÃ©cessitant du temps
- **PortÃ©e** : IntÃ©grations externes, traitements lourds
- **Exemples** : Upload de fichiers, gÃ©nÃ©ration de rapports

### Tests de services externes (`@pytest.mark.external`)
- **Objectif** : Tester les intÃ©grations tierces
- **PortÃ©e** : Stripe, OpenAI, gÃ©olocalisation
- **Exemples** : Paiements, IA, cartes

## ğŸ”§ Configuration

### Fichier `conftest.py`
Contient la configuration globale et les fixtures communes :
- Configuration de l'application de test
- Fixtures de base de donnÃ©es
- Mocks des services externes
- DonnÃ©es de test

### Fichier `pytest.ini`
Configuration pytest avec :
- Marqueurs personnalisÃ©s
- Options par dÃ©faut
- Filtres d'avertissements

### Variables d'environnement
```bash
# Configuration de test
FLASK_ENV=testing
TESTING=true
DATABASE_URL=sqlite:///:memory:
```

## ğŸ“Š Couverture de code

### GÃ©nÃ©ration du rapport
```bash
pytest --cov=backend --cov-report=html --cov-report=term-missing
```

### Rapport HTML
Le rapport est gÃ©nÃ©rÃ© dans `htmlcov/` et peut Ãªtre ouvert dans un navigateur.

### MÃ©triques cibles
- **Couverture globale** : > 90%
- **Couverture des modÃ¨les** : > 95%
- **Couverture des services** : > 85%
- **Couverture des routes** : > 80%

## ğŸ­ Mocks et stubs

### Services externes
- **Stripe** : Mock des paiements
- **OpenAI** : Mock des rÃ©ponses IA
- **GÃ©olocalisation** : Mock des coordonnÃ©es
- **Redis** : Mock du cache
- **Celery** : Mock des tÃ¢ches asynchrones

### Base de donnÃ©es
- **SQLite en mÃ©moire** pour les tests
- **Transactions** avec rollback automatique
- **Fixtures** pour les donnÃ©es de test

## ğŸ“ Ã‰criture de tests

### Structure recommandÃ©e
```python
class TestUserService:
    """Tests pour le service utilisateur"""
    
    def test_create_user_success(self, db_session):
        """Test la crÃ©ation rÃ©ussie d'un utilisateur"""
        # Arrange
        user_data = {...}
        
        # Act
        result = user_service.create_user(user_data)
        
        # Assert
        assert result.username == user_data['username']
        assert result.is_active is True
    
    def test_create_user_duplicate_email(self, db_session, existing_user):
        """Test la crÃ©ation avec un email dupliquÃ©"""
        # Arrange
        user_data = {'email': existing_user.email}
        
        # Act & Assert
        with pytest.raises(ValidationError):
            user_service.create_user(user_data)
```

### Bonnes pratiques
1. **Noms descriptifs** : `test_create_user_with_valid_data_succeeds`
2. **Une assertion par test** : Plus facile Ã  dÃ©boguer
3. **Fixtures rÃ©utilisables** : Ã‰viter la duplication
4. **Mocks appropriÃ©s** : Isoler les dÃ©pendances
5. **DonnÃ©es de test rÃ©alistes** : Ã‰viter les valeurs magiques

### Assertions recommandÃ©es
```python
# Ã‰galitÃ©
assert result.status_code == 200
assert user.username == "testuser"

# Exceptions
with pytest.raises(ValidationError):
    service.process_data(invalid_data)

# Collections
assert len(users) == 3
assert "admin" in [u.role for u in users]

# Types
assert isinstance(result, dict)
assert hasattr(user, 'email')
```

## ğŸš¨ Gestion des erreurs

### Tests d'erreurs attendues
```python
def test_invalid_input_raises_error(self):
    """Test que les donnÃ©es invalides lÃ¨vent une erreur"""
    with pytest.raises(ValidationError) as exc_info:
        validate_user_data(invalid_data)
    
    assert "email" in str(exc_info.value)
    assert exc_info.value.status_code == 400
```

### Tests de robustesse
```python
def test_service_handles_database_error(self, mock_db):
    """Test que le service gÃ¨re les erreurs de base de donnÃ©es"""
    mock_db.side_effect = DatabaseError("Connection failed")
    
    with pytest.raises(ServiceError):
        user_service.get_user(1)
```

## ğŸ“ˆ Performance

### Tests de performance
```python
@pytest.mark.benchmark
def test_search_performance(self, benchmark, sample_data):
    """Test les performances de la recherche"""
    result = benchmark(search_service.search, "test")
    assert len(result) > 0
```

### Tests de charge
```python
@pytest.mark.slow
def test_concurrent_users(self):
    """Test la gestion de plusieurs utilisateurs simultanÃ©s"""
    # Simuler 100 utilisateurs simultanÃ©s
    results = []
    for i in range(100):
        result = user_service.create_user(f"user{i}")
        results.append(result)
    
    assert len(results) == 100
```

## ğŸ” DÃ©bogage

### Mode verbeux
```bash
pytest -v -s --tb=long
```

### ArrÃªt au premier Ã©chec
```bash
pytest -x
```

### Tests spÃ©cifiques
```bash
pytest -k "test_user"  # Tests contenant "test_user"
pytest tests/test_api.py::TestAuthAPI::test_login  # Test spÃ©cifique
```

### Logs de dÃ©bogage
```python
import logging
logging.basicConfig(level=logging.DEBUG)

def test_with_logs():
    logging.debug("DÃ©but du test")
    # ... test ...
    logging.debug("Fin du test")
```

## ğŸš€ IntÃ©gration continue

### GitHub Actions
```yaml
- name: Run tests
  run: |
    pip install -r requirements-test.txt
    pytest --cov=backend --cov-report=xml
```

### Pre-commit hooks
```yaml
- repo: local
  hooks:
    - id: pytest
      name: pytest
      entry: pytest
      language: system
      pass_filenames: false
      always_run: true
```

## ğŸ“š Ressources

- [Documentation pytest](https://docs.pytest.org/)
- [Guide des fixtures](https://docs.pytest.org/en/stable/fixture.html)
- [ParamÃ©trage des tests](https://docs.pytest.org/en/stable/parametrize.html)
- [Mocks et patches](https://docs.python.org/3/library/unittest.mock.html)

## ğŸ¤ Contribution

### Ajouter de nouveaux tests
1. CrÃ©er le fichier de test dans le bon rÃ©pertoire
2. Suivre la convention de nommage
3. Ajouter les marqueurs appropriÃ©s
4. Documenter les cas de test complexes
5. VÃ©rifier la couverture

### Maintenir les tests existants
1. Mettre Ã  jour les fixtures si nÃ©cessaire
2. Adapter les mocks aux changements d'API
3. VÃ©rifier que les tests restent pertinents
4. Nettoyer les donnÃ©es de test obsolÃ¨tes

---

**Note** : Les tests sont essentiels pour maintenir la qualitÃ© du code. ExÃ©cutez-les rÃ©guliÃ¨rement et maintenez une couverture Ã©levÃ©e !
